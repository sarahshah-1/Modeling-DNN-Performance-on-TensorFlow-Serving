# Modeling-DNN-Performance-on-TensorFlow-Serving

This repository investigates the performance of Machine Learning (ML) applications under simultaneously varying resources and workloads, particularly on TensorFlow Serving. Although ML and Artificial Intelligence (AI) have been researched since the early 1950s, their recent popularity is due to the tremendous growth of data generated by modern computing systems, which traditional statistical methods cannot handle. As a result, many businesses are adopting solutions incorporating complex ML algorithms to analyze and model their data, and TensorFlow Serving has emerged as a popular platform for hosting ML models in server environments due to its ease of use and flexibility.

To ensure optimal performance, businesses require performance insights about their ML applications under realistic user workloads. This helps in allocating appropriate resources that will ensure that the response time of an ML application does not exceed a predefined threshold while serving a particular workload. However, despite the popularity of ML models in modern business solutions, there is limited research on performance prediction of ML inference time under resource and workload constraints. This proposal identifies and addresses these gaps by developing a comprehensive dataset capturing the behavior of well-known Deep Neural Networks (DNNs) - a popular type of ML algorithms - under a variety of simultaneously changing resource constraints and user workloads. The collected dataset is used to study the impact of individual and combined external variables on the response times of multiple types of DNNs to build a single performance model for predicting the response times of multiple DNNs. A generalized performance model that is able to offer insights for multiple ML model types is useful because businesses often have different ML models deployed to complete different business tasks and thus having a single performance model becomes more pragmatic. Our modeling dataset is only comprised of metrics that are available prior to when a DNN is ready for inference and hence has enabled the development of a performance model that can offer performance insights without the need to collect data during inference. This can help system administrators make quick decisions prior to serving the model and also prevent any overhead caused by using our performance model.

This repository implements and evaluates our performance modeling process in detail and also explores data pruning techniques to reduce the training effort of the performance model without impacting its prediction error. The developed performance models are evaluated using completely unseen scenarios to test their ability to predict performance in unforeseen workloads and resource configurations. The results provide valuable insights into how to leverage ML models to effectively offer generalized predictions for the inference time of various DNNs running on top of TensorFlow Serving in dynamic settings. 
